# -*- coding: utf-8 -*-
"""American Viewpoint Verbatims - Streamlined Version

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w14TiAdLR303h31G9IsxSwjwvLWQo8n5

## American Viewpoint Coded Verbatims

1. Add API key each session (re-enter each session for security purposes).
2. Upload CSV of open-ended responses (supports multiple question columns).
3. Sends each question column to chosen LLM (via OpenRouter) with prompt.
4. Makes categories with explanations and codes verbatims into categories.
5. Saves and downloads results.

## Step 1 - Install Dependencies and Configure
"""

# Commented out IPython magic to ensure Python compatibility.
# 1) Install dependencies (Colab-safe)
# %pip -q install --upgrade openai python-dotenv tqdm
# %pip -q install 'pandas==2.2.2'

import pandas as _pd
import os
from getpass import getpass
from openai import OpenAI

# Enter your OpenRouter API key. You can create one at https://openrouter.ai/keys
if not os.getenv("OPENROUTER_API_KEY"):
    os.environ["OPENROUTER_API_KEY"] = getpass("Enter your OPENROUTER_API_KEY: ")

# OpenAI Key: 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

# Optional but recommended: set a referer and title to appear in your OpenRouter dashboard
OPENROUTER_REF = os.getenv("OPENROUTER_REF", "https://colab.research.google.com/")
OPENROUTER_TITLE = os.getenv("OPENROUTER_TITLE", "Survey Theme Extractor")

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.environ["OPENROUTER_API_KEY"],
    default_headers={
        "HTTP-Referer": OPENROUTER_REF,  # site URL
        "X-Title": OPENROUTER_TITLE,     # site name
    },
)

MODEL = "openai/gpt-5"
#print(f"Using model: {MODEL}")

"""## Step 2 - Upload CSV"""

import pandas as pd
import re
try:
    from google.colab import files  # type: ignore
    _in_colab = True
except Exception:
    _in_colab = False

if _in_colab:
    uploaded = files.upload()
    if not uploaded:
        raise ValueError("No file uploaded.")
    CSV_PATH = next(iter(uploaded.keys()))
else:
    # If not in Colab, set CSV_PATH manually (e.g., './data/responses.csv')
    CSV_PATH = "./responses.csv"

df = pd.read_csv(CSV_PATH, encoding="latin1")

"""## Step 3 - Define Prompt & Helpers"""

import json
from tqdm import tqdm

USER_PROMPT = (
    "Your role: You are a senior survey research analyst.\n"
    "Your task: read the list of open-ended responses to the survey questions in the attached csv and identify the key themes. It is crucial that every ParticipantID goes into at least one theme category for each question. You may include categories for 'Other', 'Don't Know', and 'Refused' if needed.\n"
    "Instructions: \n"
    "1) Identify 6-9 themes that capture the main ideas expressed. \n"
    "2) For each theme, provide: \n"
    "- ThemeLabel (3–5 neutral words) \n"
    "- Definition (short, factual) \n"
    "- RepresentativeKeywords (5–10 indicative words/phrases) \n"
    "- ParticipantID (row numbers that correspond to the theme)\n"
    "3) Output ONLY JSON in this format: \n"
    "[ \n"
    "{{ \n"
    "\"ThemeLabel\": \"Theme Name\", \n"
    "\"Definition\": \"Short definition.\", \n"
    "\"RepresentativeKeywords\": [\"keyword1\", \"keyword2\"],\n"
    "\"ParticipantID\": [\"row number1\", \"row number2\"]\n"
    " }}\n"
    "]"
)

ID_COLUMN = "respid"  # change if your unique row ID column is named differently
if ID_COLUMN not in df.columns:
    raise ValueError(f"Expected an ID column named '{ID_COLUMN}' in the CSV.")

QUESTION_COLS = [c for c in df.columns if re.match(r"^q\d+", str(c))]
if not QUESTION_COLS:
    raise ValueError("No question columns found matching the pattern Q\\d+ (e.g., Q24, Q25).")


def build_payload_for_column(col: str, max_chars: int = 120_000) -> str:
    lines = []
    total = 0
    for _, row in df[[ID_COLUMN, col]].dropna(subset=[col]).iterrows():
        rid = str(row[ID_COLUMN])
        txt = str(row[col]).replace("\n", " ").strip()
        line = f"record={rid} | response={txt}"
        total += len(line) + 1
        if total > max_chars:
            break
        lines.append(line)
    return "\n".join(lines)

def llm_theme_extract(column_name: str, model: str) -> str:
    payload = build_payload_for_column(column_name)
    messages = [
        {"role": "system", "content": "You are a precise, compliance-focused data analyst. Output strictly valid JSON with no commentary."},
        {"role": "user", "content": (
            "Analyze the following open-ended responses for column '" + column_name + "'.\n\n" +
            USER_PROMPT + "\n\n" +
            "Use the 'record' value as ParticipantID.\n\n" +
            "RESPONSES (one per line):\n" + payload
        )}
    ]
    try:
        r = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.2,
        )
        return r.choices[0].message.content
    except Exception as e:
        raise RuntimeError(f"OpenRouter request failed for {column_name}: {e}")

def parse_json_maybe(text: str):
    if text is None:
        return []
    t = text.strip()
    if t.startswith("```"):
        t = t.strip("`\n ")
        if t.lower().startswith("json"):
            t = t[4:].strip()
    try:
        return json.loads(t)
    except Exception:
        i = t.find("[")
        j = t.rfind("]")
        if i != -1 and j != -1 and j > i:
            try:
                return json.loads(t[i:j+1])
            except Exception:
                pass
        raise

"""## Step 4 - Run Helper Function

Adds placeholders for blank counts/cells
"""

import re, math

# Treat these as "no response"
PLACEHOLDERS = {
    "", " "
    #"na", "n/a", "none", "no response", "no comment", "nil", ".", "-", "--"
}

def _normalize_placeholder(s: str) -> str:
    # collapse non-letters/numbers to spaces, lowercase, trim
    return re.sub(r"[^0-9A-Za-z]+", " ", (s or "")).strip().lower()

def is_meaningful(text: str, min_chars: int = 3, min_tokens: int = 1) -> bool:
    s = (text or "").strip()
    if len(s) < min_chars:
        return False
    norm = _normalize_placeholder(s)
    if norm in PLACEHOLDERS:
        return False
    # token-based gate if you have tiktoken; else approx 1 token per 4 chars
    try:
        import tiktoken
        ENC = tiktoken.get_encoding("o200k_base")
        toks = len(ENC.encode(s))
    except Exception:
        toks = math.ceil(len(s) / 4)
    return toks >= min_tokens

    # Sanity check: how many blanks/placeholder responses per question?
    for col in QUESTION_COLS:
        total = int(df[col].notna().sum())
        blanks = sum(1 for v in df[col].dropna() if not is_meaningful(str(v)))
        pct = (blanks / total * 100) if total else 0
        print(f"{col}: {blanks}/{total} skipped (~{pct:.1f}%)")

"""## Step 5 - Theme Extractor Per Question Column (simple, one-shot version)


"""

from tqdm import tqdm
all_results = {}
for col in tqdm(QUESTION_COLS):
    raw = llm_theme_extract(col, MODEL)
    try:
        parsed = parse_json_maybe(raw)
    except Exception as e:
        print(f"\n⚠️ JSON parse failed for {col}. Saving raw text for inspection.")
        parsed = {"_raw": raw}
    all_results[col] = parsed

json_path = "openrouter_themes_by_question.json"
with open(json_path, "w", encoding="utf-8") as f:
    json.dump(all_results, f, ensure_ascii=False, indent=2)

print(f"Saved: {json_path}")

"""## Step 6 - Flatten to CSV"""

import pandas as pd
rows = []
def _stringify_list(x):
    if isinstance(x, list):
        return "; ".join(map(str, x))
    return ""

for qcol, content in all_results.items():
    if isinstance(content, list):
        for t in content:
            rows.append({
                "Question": qcol,
                "ThemeLabel": t.get("ThemeLabel", ""),
                "Definition": t.get("Definition", ""),
                "RepresentativeKeywords": _stringify_list(t.get("RepresentativeKeywords", [])),
                "ParticipantID": _stringify_list(t.get("ParticipantID", [])),
            })
    else:
        rows.append({
            "Question": qcol,
            "ThemeLabel": "_parse_error",
            "Definition": "Raw LLM text saved to JSON file.",
            "RepresentativeKeywords": "",
            "ParticipantID": "",
        })

out_csv = "openrouter_themes_by_question.csv"
pd.DataFrame(rows).to_csv(out_csv, index=False, encoding="utf-8")
print(f"Saved: {out_csv}")

"""## Step 7 - Create Coded Matrix"""

import pandas as pd, re

def _slug(s: str) -> str:
    s = str(s or '').strip()
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"[^0-9A-Za-z _-]", "", s)   # keep letters, numbers, space, '_' and '-'
    return s

# Optional: standardize model labels to your preferred headers
LABEL_MAP = {
    # "Low product quality": "Product Quality",
    # "Cheap products and deals": "Low Price",
}

# Build a user_id column (to match your example file)
user_ids = df[ID_COLUMN].astype(str).unique().tolist()
codes = pd.DataFrame({"user_id": user_ids})
added_cols = []

for qcol, themes in all_results.items():
    if not isinstance(themes, list):
        continue
    for t in themes:
        raw_label = t.get("ThemeLabel", "")
        label = LABEL_MAP.get(raw_label, raw_label)
        label = _slug(label)
        if not label:
            continue

        colname = f"{qcol}_{label}"
        # ensure uniqueness if the same name appears twice
        if colname in codes.columns:
            base, k = colname, 2
            while colname in codes.columns:
                colname = f"{base}_{k}"
                k += 1

        ids = set(map(str, t.get("ParticipantID", [])))
        codes[colname] = [1 if str(uid) in ids else 0 for uid in user_ids]
        added_cols.append(colname)

# Sort columns by question number then label for readability
q_order = sorted(set(c.split('_', 1)[0] for c in added_cols),
                 key=lambda q: (int(re.findall(r"\d+", q)[0]) if re.findall(r"\d+", q) else 0, q))
sorted_cols = ["user_id"]
for q in q_order:
    sorted_cols += sorted([c for c in added_cols if c.startswith(q + "_")])
codes = codes[sorted_cols]

out_codes_csv = "openrouter_codes_by_question.csv"
codes.to_csv(out_codes_csv, index=False, encoding="utf-8")
print(f"Saved: {out_codes_csv} with shape {codes.shape}")

"""## Step 8 - Download Results"""

try:
    from google.colab import files  # type: ignore
    #files.download("openrouter_themes_by_question.json")
    files.download("openrouter_themes_by_question.csv")
    files.download("openrouter_codes_by_question.csv")
except Exception as _:
    print("Not running in Colab, skipping auto-download.")